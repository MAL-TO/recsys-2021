{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compact-homeless",
   "metadata": {},
   "source": [
    "# Sample dataset generator\n",
    "\n",
    "Sample a big dataset to create small(er) datasets. Run this notebook on the cluster.\n",
    "\n",
    "Produces a folder with parts of a parquet file, located on HDFS at `OUTPUT_PATH/OUTPUT_DIR`.\n",
    "\n",
    "To merge the parts into a single file and bring it to the local filesystem, use `hdfs dfs -getmerge OUTPUT_PATH/OUTPUT_DIR <filename>.parquet`.\n",
    "\n",
    "To load the file locally, use pandas:\n",
    "```python\n",
    "df = pd.read_parquet(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adverse-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-integrity",
   "metadata": {},
   "source": [
    "G1 GC useful links:\n",
    "- https://www.oracle.com/technical-resources/articles/java/g1gc.html for overview\n",
    "- https://spark.apache.org/docs/latest/tuning.html#memory-management-overview for description of tuning options\n",
    "- https://spark.apache.org/docs/latest/configuration.html#runtime-environment for JavaOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "organized-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set G1  GC as garbage collector\n",
    "spark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\n",
    "spark.conf.set(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\")\n",
    "\n",
    "# Set heap of a G1 region to maximum size\n",
    "spark.conf.set(\"spark.driver.extraJavaOptions\", \"XX:G1HeapRegionSize=32m\")\n",
    "\n",
    "# Increase executor memory\n",
    "spark.conf.set(\"spark.executor.memory\", \"64g\")\n",
    "\n",
    "# Logging of GC usage: debug to find the bottleneck\n",
    "spark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+PrintGCTimeStamps\")\n",
    "spark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+PrintGCDetails\")\n",
    "\n",
    "# Set logging directory\n",
    "spark.conf.set(\"spark.eventLog.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.eventLog.dir\", \"hdfs://BigDataHA/user/s277596/spark/spark2ApplicationHistory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "resistant-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"hdfs://BigDataHA/user/s277309/recsys_data/\"\n",
    "OUTPUT_PATH = \"recsys_data_sample_generated\"\n",
    "\n",
    "# If not empty, drop the specified columns from the sample.\n",
    "# Note: this does not actually drop columns. Rather, it fills\n",
    "# the column with `0` values. This is done to keep compatibility\n",
    "# with our importer\n",
    "FILL_NULL_COLUMNS = [\n",
    "    \n",
    "]\n",
    "\n",
    "NO_TIMESTAMP = False  # If True, transform timestamps into \"1\" if present, \"0\" if not present (saves disk space)\n",
    "\n",
    "TOTAL_ROWS = 747694282\n",
    "SAMPLE_ROWS = 2000000  # Number of rows to sample\n",
    "\n",
    "# If True, get a samples from a time window. This is necessary for correct training and testing conditions\n",
    "TIME_WINDOW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loved-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # Tweet features\n",
    "    \"text_tokens\",      # List[long]    Ordered list of Bert ids corresponding to Bert tokenization of Tweet text\n",
    "    \"hashtags\",         # List[string]  Tab separated list of hastags (identifiers) present in the tweet\n",
    "    \"tweet_id\",         # String        Tweet identifier (unique)\n",
    "    \"present_media\",    # List[String]  Tab separated list of media types. Media type can be in (Photo, Video, Gif)\n",
    "    \"present_links\",    # List[string]  Tab separated list of links (identifiers) included in the Tweet\n",
    "    \"present_domains\",  # List[string]  Tab separated list of domains included in the Tweet (twitter.com, dogs.com)\n",
    "    \"tweet_type\",       # String        Tweet type, can be either Retweet, Quote, Reply, or Toplevel\n",
    "    \"language\",         # String        Identifier corresponding to the inferred language of the Tweet\n",
    "    \"tweet_timestamp\",  # Long          Unix timestamp, in sec of the creation time of the Tweet\n",
    "    \n",
    "    # Engaged-with User (i.e., Engagee) Features\n",
    "    \"engaged_with_user_id\",                 # String    User identifier\n",
    "    \"engaged_with_user_follower_count\",     # Long      Number of followers of the user\n",
    "    \"engaged_with_user_following_count\",    # Long      Number of accounts the user is following\n",
    "    \"engaged_with_user_is_verified\",        # Bool      Is the account verified?\n",
    "    \"engaged_with_user_account_creation\",   # Long      Unix timestamp, in seconds, of the creation time of the account\n",
    "    \n",
    "    # Engaging User (i.e., Engager) Features\n",
    "    \"engaging_user_id\",                     # String    User identifier   \n",
    "    \"engaging_user_follower_count\",         # Long      Number of followers of the user\n",
    "    \"engaging_user_following_count\",        # Long      Number of accounts the user is following\n",
    "    \"engaging_user_is_verified\",            # Bool      Is the account verified?\n",
    "    \"engaging_user_account_creation\",       # Long      Unix timestamp, in seconds, of the creation time of the account\n",
    "    \n",
    "    # Engagement features\n",
    "    \"engagee_follows_engager\"   # Bool  Engagee follows engager?\n",
    "]\n",
    "\n",
    "features_idx = dict(zip(features, range(len(features))))\n",
    "\n",
    "labels_idx = {\n",
    "    # Engagement features (cont.)\n",
    "    \"reply_timestamp\": 20,                  # Long      Unix timestamp (in seconds) of one of the replies, if there is at least one\n",
    "    \"retweet_timestamp\": 21,                # Long      Unix timestamp (in seconds) of the retweet by the engaging user, if there is at least one\n",
    "    \"retweet_with_comment_timestamp\": 22,   # Long      Unix timestamp (in seconds) of one of the retweet with comment by the engaging user, if there is at least one\n",
    "    \"like_timestamp\": 23                    # Long      Unix timestamp (in seconds) of the like by the engaging user, if they liked the tweet\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confirmed-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = [\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"int\",\n",
    "    \"string\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"boolean\",\n",
    "    \"int\",\n",
    "    \"string\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"boolean\",\n",
    "    \"int\",\n",
    "    \"boolean\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "favorite-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "lines_rdd = sc.textFile(INPUT_PATH)\n",
    "\n",
    "# Split each line\n",
    "# Fields in each data entry are separated by the 1 character (0x31 in UTF-8).\n",
    "# https://recsys-twitter.com/code/snippets\n",
    "fields_rdd = lines_rdd.map(lambda line: line.strip().split(\"\\x01\"))\n",
    "\n",
    "# Adapted from: https://github.com/MAL-TO/recsys-2021/blob/a047fba6385453b90f6754ac7ebe36eaf622cb2c/andrea/recsys-2021/src/data/make_dataset.py\n",
    "# Eventually delete timestamps for targets and put 1 if a timestamp is present, 0 otherwise\n",
    "def timestamp_to_bool(l):\n",
    "    \"\"\"Transform targets into either 1 or 0, based on whether a timestamp is present or not in `label_key`\"\"\"\n",
    "    for label_key in labels_idx:\n",
    "        l[labels_idx[label_key]] = int(len(l[labels_idx[label_key]]) > 0)\n",
    "    return l\n",
    "\n",
    "if NO_TIMESTAMP:\n",
    "    fields_rdd = fields_rdd.map(lambda line: timestamp_to_bool(line))\n",
    "\n",
    "# Transform to Spark dataframe\n",
    "schema = features + list(labels_idx.keys())  # Column names\n",
    "df = spark.createDataFrame(fields_rdd, schema)\n",
    "\n",
    "# Eventually drop some columns from the dataframe\n",
    "for col_to_drop in FILL_NULL_COLUMNS:\n",
    "    df = df.withColumn(col_to_drop, lit(0))\n",
    "    \n",
    "for i in range(len(dtypes)):\n",
    "    dtype = dtypes[i]\n",
    "    field = schema[i]\n",
    "    df = df.withColumn(field, df[field].cast(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Sample data\n",
    "# OUTPUT_DIR = f\"sample_{SAMPLE_ROWS/TOTAL_ROWS:.4f}\"\n",
    "# if TIME_WINDOW:\n",
    "#     # Reversing the order since first samples are less meaningful (see stationarity eda)\n",
    "#     df.createOrReplaceTempView(\"dataframe\")\n",
    "#     query = f\"SELECT * FROM dataframe ORDER BY tweet_timestamp DESC LIMIT {SAMPLE_ROWS}\"\n",
    "#     sample_df = spark.sql(query)\n",
    "# else:\n",
    "#     sample_df = df.sample(withReplacement=False, fraction=SAMPLE_ROWS/TOTAL_ROWS)\n",
    "# sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sample data\n",
    "OUTPUT_DIR = f\"sample_{SAMPLE_ROWS/TOTAL_ROWS:.4f}\"\n",
    "\n",
    "df.createOrReplaceTempView(\"dataframe\")\n",
    "query = \"SELECT * FROM dataframe ORDER BY tweet_timestamp\"\n",
    "sorted_df = spark.sql(query)\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sorted_df.createOrReplaceTempView(\"sorted\")\n",
    "query = f\"SELECT * FROM sorted LIMIT {SAMPLE_ROWS}\"\n",
    "sample_df = spark.sql(query)\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to parquet file\n",
    "sample_df.write.parquet(os.path.join(OUTPUT_PATH, OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.join(OUTPUT_PATH, OUTPUT_DIR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
