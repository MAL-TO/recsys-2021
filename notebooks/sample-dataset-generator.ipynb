{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intended-photographer",
   "metadata": {},
   "source": [
    "# Sample dataset generator\n",
    "\n",
    "Sample a big dataset to create small(er) datasets. Run this notebook on the cluster.\n",
    "\n",
    "Produces a folder with parts of a parquet file, located on HDFS at `OUTPUT_PATH/OUTPUT_DIR`.\n",
    "\n",
    "To merge the parts into a single file and bring it to the local filesystem, use `hdfs dfs -getmerge OUTPUT_PATH/OUTPUT_DIR <filename>.parquet`.\n",
    "\n",
    "To load the file locally, use pandas:\n",
    "```python\n",
    "df = pd.read_parquet(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lovely-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"hdfs://BigDataHA/user/s277309/recsys_data/\"\n",
    "OUTPUT_PATH = \"recsys_data_sample_generated\"\n",
    "\n",
    "# If not empty, drop the specified columns from the sample.\n",
    "# Note: this does not actually drop columns. Rather, it fills\n",
    "# the column with `0` values. This is done to keep compatibility\n",
    "# with our importer\n",
    "FILL_NULL_COLUMNS = [\n",
    "    \n",
    "]\n",
    "\n",
    "NO_TIMESTAMP = False  # If True, transform timestamps into \"1\" if present, \"0\" if not present (saves disk space)\n",
    "\n",
    "TOTAL_ROWS = 747694282\n",
    "SAMPLE_ROWS = 200000  # Number of rows to sample. Needed only if TIME_WINDOW is set to None\n",
    "\n",
    "# If you want elements sorted by timestamp, require a dataset in terms of time window, expressed in seconds\n",
    "# e.g. to have samples from the last 2 hours of the dataset, set TIME_WINDOW = 2*60*60 = 7200\n",
    "# Else, set this value to None, as it will be used as falsy\n",
    "# This mess is a workaround to dataframe.limit() issues which I couldn't overcome\n",
    "TIME_WINDOW = 10800 # [None]\n",
    "LAST_TIMESTAMP = 1614211199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "headed-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # Tweet features\n",
    "    \"text_tokens\",      # List[long]    Ordered list of Bert ids corresponding to Bert tokenization of Tweet text\n",
    "    \"hashtags\",         # List[string]  Tab separated list of hastags (identifiers) present in the tweet\n",
    "    \"tweet_id\",         # String        Tweet identifier (unique)\n",
    "    \"present_media\",    # List[String]  Tab separated list of media types. Media type can be in (Photo, Video, Gif)\n",
    "    \"present_links\",    # List[string]  Tab separated list of links (identifiers) included in the Tweet\n",
    "    \"present_domains\",  # List[string]  Tab separated list of domains included in the Tweet (twitter.com, dogs.com)\n",
    "    \"tweet_type\",       # String        Tweet type, can be either Retweet, Quote, Reply, or Toplevel\n",
    "    \"language\",         # String        Identifier corresponding to the inferred language of the Tweet\n",
    "    \"tweet_timestamp\",  # Long          Unix timestamp, in sec of the creation time of the Tweet\n",
    "    \n",
    "    # Engaged-with User (i.e., Engagee) Features\n",
    "    \"engaged_with_user_id\",                 # String    User identifier\n",
    "    \"engaged_with_user_follower_count\",     # Long      Number of followers of the user\n",
    "    \"engaged_with_user_following_count\",    # Long      Number of accounts the user is following\n",
    "    \"engaged_with_user_is_verified\",        # Bool      Is the account verified?\n",
    "    \"engaged_with_user_account_creation\",   # Long      Unix timestamp, in seconds, of the creation time of the account\n",
    "    \n",
    "    # Engaging User (i.e., Engager) Features\n",
    "    \"engaging_user_id\",                     # String    User identifier   \n",
    "    \"engaging_user_follower_count\",         # Long      Number of followers of the user\n",
    "    \"engaging_user_following_count\",        # Long      Number of accounts the user is following\n",
    "    \"engaging_user_is_verified\",            # Bool      Is the account verified?\n",
    "    \"engaging_user_account_creation\",       # Long      Unix timestamp, in seconds, of the creation time of the account\n",
    "    \n",
    "    # Engagement features\n",
    "    \"engagee_follows_engager\"   # Bool  Engagee follows engager?\n",
    "]\n",
    "\n",
    "features_idx = dict(zip(features, range(len(features))))\n",
    "\n",
    "labels_idx = {\n",
    "    # Engagement features (cont.)\n",
    "    \"reply_timestamp\": 20,                  # Long      Unix timestamp (in seconds) of one of the replies, if there is at least one\n",
    "    \"retweet_timestamp\": 21,                # Long      Unix timestamp (in seconds) of the retweet by the engaging user, if there is at least one\n",
    "    \"retweet_with_comment_timestamp\": 22,   # Long      Unix timestamp (in seconds) of one of the retweet with comment by the engaging user, if there is at least one\n",
    "    \"like_timestamp\": 23                    # Long      Unix timestamp (in seconds) of the like by the engaging user, if they liked the tweet\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continued-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = [\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "    \"int\",\n",
    "    \"string\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"boolean\",\n",
    "    \"int\",\n",
    "    \"string\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"boolean\",\n",
    "    \"int\",\n",
    "    \"boolean\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "earned-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "lines_rdd = sc.textFile(INPUT_PATH)\n",
    "\n",
    "# Split each line\n",
    "# Fields in each data entry are separated by the 1 character (0x31 in UTF-8).\n",
    "# https://recsys-twitter.com/code/snippets\n",
    "fields_rdd = lines_rdd.map(lambda line: line.strip().split(\"\\x01\"))\n",
    "\n",
    "# Adapted from: https://github.com/MAL-TO/recsys-2021/blob/a047fba6385453b90f6754ac7ebe36eaf622cb2c/andrea/recsys-2021/src/data/make_dataset.py\n",
    "# Eventually delete timestamps for targets and put 1 if a timestamp is present, 0 otherwise\n",
    "def timestamp_to_bool(l):\n",
    "    \"\"\"Transform targets into either 1 or 0, based on whether a timestamp is present or not in `label_key`\"\"\"\n",
    "    for label_key in labels_idx:\n",
    "        l[labels_idx[label_key]] = int(len(l[labels_idx[label_key]]) > 0)\n",
    "    return l\n",
    "\n",
    "if NO_TIMESTAMP:\n",
    "    fields_rdd = fields_rdd.map(lambda line: timestamp_to_bool(line))\n",
    "\n",
    "# Transform to Spark dataframe\n",
    "schema = features + list(labels_idx.keys())  # Column names\n",
    "df = spark.createDataFrame(fields_rdd, schema)\n",
    "\n",
    "# Eventually drop some columns from the dataframe\n",
    "for col_to_drop in FILL_NULL_COLUMNS:\n",
    "    df = df.withColumn(col_to_drop, lit(0))\n",
    "    \n",
    "for i in range(len(dtypes)):\n",
    "    dtype = dtypes[i]\n",
    "    field = schema[i]\n",
    "    df = df.withColumn(field, df[field].cast(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sample data\n",
    "OUTPUT_DIR = f\"sample_{SAMPLE_ROWS/TOTAL_ROWS:.4f}\"\n",
    "if TIME_WINDOW:\n",
    "    # Reversing the order since first samples are less meaningful (see stationarity eda)\n",
    "    sample_df = df.filter(df.tweet_timestamp > (LAST_TIMESTAMP - TIME_WINDOW))\n",
    "    SAMPLE_ROWS = sample_df.count()\n",
    "    OUTPUT_DIR = f\"sample_{SAMPLE_ROWS/TOTAL_ROWS:.4f}\"\n",
    "else:\n",
    "    OUTPUT_DIR = f\"sample_{SAMPLE_ROWS/TOTAL_ROWS:.4f}\"\n",
    "    sample_df = df.sample(withReplacement=False, fraction=SAMPLE_ROWS/TOTAL_ROWS)\n",
    "\n",
    "# Write to parquet file\n",
    "sample_df.write.parquet(os.path.join(OUTPUT_PATH, OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "strategic-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recsys_data_sample_generated/sample_0.0047\n"
     ]
    }
   ],
   "source": [
    "print(os.path.join(OUTPUT_PATH, OUTPUT_DIR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
