{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sustained-connecticut",
   "metadata": {},
   "source": [
    "# Test sampler\n",
    "\n",
    "Sample a big dataset to create small(er) datasets. Outputs $n$ pairs of {train, test} datasets\n",
    "\n",
    "Each dataset (train or test) is stored as a folder with parts of a parquet file, located on HDFS at `OUTPUT_PATH`.\n",
    "\n",
    "To merge the parts into a single file and bring it to the local filesystem, run the last cell of the notebook.\n",
    "\n",
    "Run this notebook on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "physical-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "increasing-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # Tweet features\n",
    "    \"text_tokens\",      # List[long]    Ordered list of Bert ids corresponding to Bert tokenization of Tweet text\n",
    "    \"hashtags\",         # List[string]  Tab separated list of hastags (identifiers) present in the tweet\n",
    "    \"tweet_id\",         # String        Tweet identifier (unique)\n",
    "    \"present_media\",    # List[String]  Tab separated list of media types. Media type can be in (Photo, Video, Gif)\n",
    "    \"present_links\",    # List[string]  Tab separated list of links (identifiers) included in the Tweet\n",
    "    \"present_domains\",  # List[string]  Tab separated list of domains included in the Tweet (twitter.com, dogs.com)\n",
    "    \"tweet_type\",       # String        Tweet type, can be either Retweet, Quote, Reply, or Toplevel\n",
    "    \"language\",         # String        Identifier corresponding to the inferred language of the Tweet\n",
    "    \"tweet_timestamp\",  # Long          Unix timestamp, in sec of the creation time of the Tweet\n",
    "    \n",
    "    # Engaged-with User (i.e., Engagee) Features\n",
    "    \"engaged_with_user_id\",                 # String    User identifier\n",
    "    \"engaged_with_user_follower_count\",     # Long      Number of followers of the user\n",
    "    \"engaged_with_user_following_count\",    # Long      Number of accounts the user is following\n",
    "    \"engaged_with_user_is_verified\",        # Bool      Is the account verified?\n",
    "    \"engaged_with_user_account_creation\",   # Long      Unix timestamp, in seconds, of the creation time of the account\n",
    "    \n",
    "    # Engaging User (i.e., Engager) Features\n",
    "    \"engaging_user_id\",                     # String    User identifier   \n",
    "    \"engaging_user_follower_count\",         # Long      Number of followers of the user\n",
    "    \"engaging_user_following_count\",        # Long      Number of accounts the user is following\n",
    "    \"engaging_user_is_verified\",            # Bool      Is the account verified?\n",
    "    \"engaging_user_account_creation\",       # Long      Unix timestamp, in seconds, of the creation time of the account\n",
    "    \n",
    "    # Engagement features\n",
    "    \"engagee_follows_engager\"   # Bool  Engagee follows engager?\n",
    "]\n",
    "\n",
    "features_idx = dict(zip(features, range(len(features))))\n",
    "\n",
    "labels_idx = {\n",
    "    # Engagement features (cont.)\n",
    "    \"reply_timestamp\": 20,                  # Long      Unix timestamp (in seconds) of one of the replies, if there is at least one\n",
    "    \"retweet_timestamp\": 21,                # Long      Unix timestamp (in seconds) of the retweet by the engaging user, if there is at least one\n",
    "    \"retweet_with_comment_timestamp\": 22,   # Long      Unix timestamp (in seconds) of one of the retweet with comment by the engaging user, if there is at least one\n",
    "    \"like_timestamp\": 23                    # Long      Unix timestamp (in seconds) of the like by the engaging user, if they liked the tweet\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "divine-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"hdfs://BigDataHA/user/s277309/recsys_data/\"\n",
    "\n",
    "OUTPUT_PATH = \"recsys_data_sample_generated\"\n",
    "\n",
    "# Filter dataset Full for tweets before a given time in the middle,\n",
    "# then from it sample one dataset A, save it. Extract list of\n",
    "# engaging users from dataset A. Filter Full data for tweets\n",
    "# AFTER the given time in the middle. Use this filtered dataset\n",
    "# to further apply a filter and only get rows where engaging user\n",
    "# is inside engaging users list. Then sample from this dataset\n",
    "# the test set. End of story\n",
    "# 5x 1mil train, 100k test\n",
    "\n",
    "# Timestamp to split between training set and test set\n",
    "SPLIT_TIMESTAMP = 1613602800  # 18 Feb 2021 (max: 1614211199 min: 1612396800)\n",
    "\n",
    "# Number of rows to extract from the full dataset.\n",
    "# TRAIN_ROWS (training set) and TEST_ROWS (test set)\n",
    "TRAIN_ROWS = 10000\n",
    "TEST_ROWS = 2000\n",
    "\n",
    "# Number of {train, test} pairs to extract\n",
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "informal-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready, train rows: 497674007 test rows: 250014492.\n",
      "CPU times: user 664 ms, sys: 466 ms, total: 1.13 s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Read data\n",
    "lines_rdd = sc.textFile(INPUT_PATH)\n",
    "\n",
    "# Split each line\n",
    "# Fields in each data entry are separated by the 1 character (0x31 in UTF-8).\n",
    "# https://recsys-twitter.com/code/snippets\n",
    "fields_rdd = lines_rdd.map(lambda line: line.strip().split(\"\\x01\"))\n",
    "\n",
    "# Assign schema\n",
    "schema = features + list(labels_idx)  # Column names\n",
    "\n",
    "# Create Spark DFs and cache\n",
    "df = spark.createDataFrame(fields_rdd, schema)\n",
    "df_train = df.filter(df.tweet_timestamp < SPLIT_TIMESTAMP).cache()\n",
    "df_test = df.filter(df.tweet_timestamp > SPLIT_TIMESTAMP).cache()\n",
    "\n",
    "TOTAL_ROWS_TRAIN = df_train.count()\n",
    "TOTAL_ROWS_TEST = df_test.count()\n",
    "\n",
    "print(f\"Dataset ready, train rows: {TOTAL_ROWS_TRAIN} test rows: {TOTAL_ROWS_TEST}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "charged-kidney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/06/07 10:18:27 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s277596/recsys_data_sample_generated/sample1' to trash at: hdfs://BigDataHA/user/s277596/.Trash/Current/user/s277596/recsys_data_sample_generated/sample1\n",
      "21/06/07 10:18:30 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s277596/recsys_data_sample_generated/sample1_test' to trash at: hdfs://BigDataHA/user/s277596/.Trash/Current/user/s277596/recsys_data_sample_generated/sample1_test\n",
      "21/06/07 10:19:12 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s277596/recsys_data_sample_generated/sample2' to trash at: hdfs://BigDataHA/user/s277596/.Trash/Current/user/s277596/recsys_data_sample_generated/sample2\n",
      "21/06/07 10:19:15 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s277596/recsys_data_sample_generated/sample2_test' to trash at: hdfs://BigDataHA/user/s277596/.Trash/Current/user/s277596/recsys_data_sample_generated/sample2_test\n",
      "21/06/07 10:19:53 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s277596/recsys_data_sample_generated/sample3' to trash at: hdfs://BigDataHA/user/s277596/.Trash/Current/user/s277596/recsys_data_sample_generated/sample3\n",
      "21/06/07 10:19:55 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s277596/recsys_data_sample_generated/sample3_test' to trash at: hdfs://BigDataHA/user/s277596/.Trash/Current/user/s277596/recsys_data_sample_generated/sample3_test\n",
      "21/06/07 10:20:32 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s277596/recsys_data_sample_generated/sample4' to trash at: hdfs://BigDataHA/user/s277596/.Trash/Current/user/s277596/recsys_data_sample_generated/sample4\n",
      "21/06/07 10:20:35 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s277596/recsys_data_sample_generated/sample4_test' to trash at: hdfs://BigDataHA/user/s277596/.Trash/Current/user/s277596/recsys_data_sample_generated/sample4_test\n",
      "21/06/07 10:21:10 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s277596/recsys_data_sample_generated/sample5' to trash at: hdfs://BigDataHA/user/s277596/.Trash/Current/user/s277596/recsys_data_sample_generated/sample5\n",
      "21/06/07 10:21:13 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s277596/recsys_data_sample_generated/sample5_test' to trash at: hdfs://BigDataHA/user/s277596/.Trash/Current/user/s277596/recsys_data_sample_generated/sample5_test\n",
      "CPU times: user 5.6 s, sys: 3.46 s, total: 9.06 s\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(FOLDS):\n",
    "    OUTPUT_DIR = f\"sample{i+1}\"\n",
    "    SEED = 153\n",
    "    \n",
    "    # Delete output directory if it already exists\n",
    "    !hdfs dfs -rm -r {OUTPUT_PATH}/{OUTPUT_DIR}\n",
    "    !hdfs dfs -rm -r {OUTPUT_PATH}/{OUTPUT_DIR}_test\n",
    "    \n",
    "    # Sample train dataset\n",
    "    df_train_sample = df_train.sample(withReplacement=False, fraction=TRAIN_ROWS/TOTAL_ROWS_TRAIN, seed=42+i+SEED)\n",
    "    \n",
    "    # Extract list of engaging users\n",
    "    df_engaging_users = df_train_sample.select(\"engaging_user_id\").distinct()\n",
    "    \n",
    "    # Get test dataset with only engaging_users inside df_engaging_users, and sample from it\n",
    "    df_test_filtered = df_test.join(df_engaging_users, on=\"engaging_user_id\", how=\"left_semi\")\n",
    "    TOTAL_ROWS_TEST_FILTERED = df_test_filtered.count()\n",
    "    df_test_sample = df_test_filtered.sample(withReplacement=False, fraction=TEST_ROWS/TOTAL_ROWS_TEST_FILTERED, seed=7357+i+SEED)\n",
    "    df_test_sample = df_test_sample.select(*schema)  # Reorder columns\n",
    "    \n",
    "    # + a small amount of random engaging users for checking purposes\n",
    "    df_test_additional = df_test.sample(withReplacement=False, fraction=2000/TOTAL_ROWS_TEST, seed=466+i+SEED)\n",
    "    df_test_additional = df_test_additional.select(*schema)  # Reorder columns\n",
    "    df_test_sample = df_test_sample.union(df_test_additional)\n",
    "    \n",
    "    # Remove duplicate data points\n",
    "    df_test_sample = df_test_sample.distinct()\n",
    "    \n",
    "    # Reorder columns\n",
    "    df_test_sample = df_test_sample.select(*schema)\n",
    "    \n",
    "    # Save everything to disk\n",
    "    df_train_sample.write.csv(os.path.join(OUTPUT_PATH, OUTPUT_DIR), sep=\"\\x01\", header=False)\n",
    "    df_test_sample.write.csv(os.path.join(OUTPUT_PATH, OUTPUT_DIR + \"_test\"), sep=\"\\x01\", header=False)\n",
    "    !hdfs dfs -getmerge {OUTPUT_PATH}/{OUTPUT_DIR} ~/recsys-2021/data/raw/{OUTPUT_DIR}\n",
    "    !hdfs dfs -getmerge {OUTPUT_PATH}/{OUTPUT_DIR}_test ~/recsys-2021/data/raw/{OUTPUT_DIR}_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
