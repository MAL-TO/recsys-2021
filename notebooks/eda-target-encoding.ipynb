{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "soviet-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "INPUT_PATH = \"hdfs://BigDataHA/user/s277309/recsys_data/\"\n",
    "TEST = True\n",
    "if TEST:\n",
    "    INPUT_PATH = \"recsys_data_sample_generated/sample_0.0003\"\n",
    "\n",
    "features = [\n",
    "    # Tweet features\n",
    "    \"text_tokens\",      # List[long]    Ordered list of Bert ids corresponding to Bert tokenization of Tweet text\n",
    "    \"hashtags\",         # List[string]  Tab separated list of hastags (identifiers) present in the tweet\n",
    "    \"tweet_id\",         # String        Tweet identifier (unique)\n",
    "    \"present_media\",    # List[String]  Tab separated list of media types. Media type can be in (Photo, Video, Gif)\n",
    "    \"present_links\",    # List[string]  Tab separated list of links (identifiers) included in the Tweet\n",
    "    \"present_domains\",  # List[string]  Tab separated list of domains included in the Tweet (twitter.com, dogs.com)\n",
    "    \"tweet_type\",       # String        Tweet type, can be either Retweet, Quote, Reply, or Toplevel\n",
    "    \"language\",         # String        Identifier corresponding to the inferred language of the Tweet\n",
    "    \"tweet_timestamp\",  # Long          Unix timestamp, in sec of the creation time of the Tweet\n",
    "    \n",
    "    # Engaged-with User (i.e., Engagee) Features\n",
    "    \"engaged_with_user_id\",                 # String    User identifier\n",
    "    \"engaged_with_user_follower_count\",     # Long      Number of followers of the user\n",
    "    \"engaged_with_user_following_count\",    # Long      Number of accounts the user is following\n",
    "    \"engaged_with_user_is_verified\",        # Bool      Is the account verified?\n",
    "    \"engaged_with_user_account_creation\",   # Long      Unix timestamp, in seconds, of the creation time of the account\n",
    "    \n",
    "    # Engaging User (i.e., Engager) Features\n",
    "    \"engaging_user_id\",                     # String    User identifier   \n",
    "    \"engaging_user_follower_count\",         # Long      Number of followers of the user\n",
    "    \"engaging_user_following_count\",        # Long      Number of accounts the user is following\n",
    "    \"engaging_user_is_verified\",            # Bool      Is the account verified?\n",
    "    \"engaging_user_account_creation\",       # Long      Unix timestamp, in seconds, of the creation time of the account\n",
    "    \n",
    "    # Engagement features\n",
    "    \"engagee_follows_engager\"   # Bool  Engagee follows engager?\n",
    "]\n",
    "\n",
    "features_idx = dict(zip(features, range(len(features))))\n",
    "\n",
    "labels_idx = {\n",
    "    # Engagement features (cont.)\n",
    "    \"reply_timestamp\": 20,                  # Long      Unix timestamp (in seconds) of one of the replies, if there is at least one\n",
    "    \"retweet_timestamp\": 21,                # Long      Unix timestamp (in seconds) of the retweet by the engaging user, if there is at least one\n",
    "    \"retweet_with_comment_timestamp\": 22,   # Long      Unix timestamp (in seconds) of one of the retweet with comment by the engaging user, if there is at least one\n",
    "    \"like_timestamp\": 23                    # Long      Unix timestamp (in seconds) of the like by the engaging user, if they liked the tweet\n",
    "}\n",
    "\n",
    "labels = [\"TARGET_\" + l for l in labels_idx.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "maritime-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mini = df.limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "provincial-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_mini.rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "chronic-carbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET_like_timestamp='', TARGET_reply_timestamp='', TARGET_retweet_timestamp='', TARGET_retweet_with_comment_timestamp='', binary_TARGET_like_timestamp=0, binary_TARGET_reply_timestamp=0, binary_TARGET_retweet_timestamp=0, binary_TARGET_retweet_with_comment_timestamp=0, engaged_with_user_account_creation='1363166784', engaged_with_user_follower_count='1201084', engaged_with_user_following_count='7', engaged_with_user_id='71B315C2C7C6494793038F07CED4C53D', engaged_with_user_is_verified='false', engagee_follows_engager='false', engaging_user_account_creation='1459508007', engaging_user_follower_count='48', engaging_user_following_count='922', engaging_user_id='6D304F25882123DB07F40C7638C9C190', engaging_user_is_verified='false', hashtags='None', language='488B32D24BD4BB44172EB981C1BCA6FA', present_domains='', present_links='', present_media='Photo\\tPhoto', text_tokens='101\\t56898\\t137\\t10144\\t12818\\t93103\\t14058\\t131\\t15127\\t21736\\t21377\\t11419\\t46128\\t21597\\t10841\\t146\\t65884\\t100\\t17339\\t13149\\t16219\\t10169\\t15127\\t10751\\t18444\\t12964\\t10245\\t100\\t14120\\t131\\t120\\t120\\t188\\t119\\t11170\\t120\\t192\\t11396\\t10115\\t104107\\t10129\\t10759\\t11779\\t11273\\t12022\\t102', tweet_id='1B6AB14A564EB2CDC08D574EB40E2B4F', tweet_timestamp='1614211065', tweet_type='Retweet')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unwindField(row, field_name):\n",
    "    dict_row = row.asDict()\n",
    "    field = dict_row.pop(field_name, None)\n",
    "    rows = []\n",
    "    if field: # If not empty\n",
    "        for el in field.split('\\t'):\n",
    "            dict_row[field_name] = el\n",
    "            rows.append(Row(**dict_row))\n",
    "\n",
    "    else:\n",
    "        dict_row[field_name] = 'None'\n",
    "        rows.append(Row(**dict_row))\n",
    "    \n",
    "    return rows\n",
    "\n",
    "unwindField(row, 'hashtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tender-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_label(el):\n",
    "    if el != '':\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "binarize = spark.udf.register(\"binary\", binarize_label, IntegerType())\n",
    "\n",
    "# Dice di non usare un for loop ma non e chiaro come aggiungere piu colonne in un colpo\n",
    "binary_labels = []\n",
    "for l in labels:\n",
    "    bin_l = \"binary_\" + l\n",
    "    df = df.withColumn(bin_l, binarize(df[l]))\n",
    "    binary_labels.append(bin_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "respected-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = ','.join([\"language\", \"tweet_id\"])\n",
    "target = \"binary_TARGET_like_timestamp\"\n",
    "df_mini.registerTempTable(\"data\")\n",
    "query = \"\"\"SELECT {}, mean({}) as conditional_probability\n",
    "            FROM data \n",
    "            GROUP BY {}\n",
    "        \"\"\".format(group, target, group)\n",
    "\n",
    "result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hispanic-manhattan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------------+\n",
      "|            language|            tweet_id|conditional_probability|\n",
      "+--------------------+--------------------+-----------------------+\n",
      "|488B32D24BD4BB441...|1B6AB14A564EB2CDC...|                    0.0|\n",
      "|488B32D24BD4BB441...|74ED8142BC78BCDAC...|                    1.0|\n",
      "|488B32D24BD4BB441...|ABD89A248E8D0AB91...|                    0.0|\n",
      "|488B32D24BD4BB441...|B24F236A2C41645BF...|                    0.0|\n",
      "|313ECD3A1E5BB0740...|ABFEBDE64CC6CB20F...|                    0.0|\n",
      "|488B32D24BD4BB441...|9BE186DDDD362DB87...|                    1.0|\n",
      "|488B32D24BD4BB441...|069CC080DD670BE8D...|                    0.0|\n",
      "|B0FA488F2911701DD...|336E75E6B67781320...|                    1.0|\n",
      "|E7F038DE3EAD397AE...|69883FF3F279C260F...|                    0.0|\n",
      "|488B32D24BD4BB441...|2F640F2A8B0B753AC...|                    0.0|\n",
      "|488B32D24BD4BB441...|73422C6330C14847E...|                    0.0|\n",
      "|488B32D24BD4BB441...|1AB5A514BEA6A12E4...|                    0.0|\n",
      "|B8B04128918BBF54E...|91645388DC6576AF9...|                    0.0|\n",
      "|488B32D24BD4BB441...|69C555FBF988E0C0C...|                    0.0|\n",
      "|B0FA488F2911701DD...|D803511AA84E96577...|                    0.0|\n",
      "|B0FA488F2911701DD...|A8291EB11D76F2E02...|                    0.0|\n",
      "|488B32D24BD4BB441...|1CF94EEE5AB5C225C...|                    0.0|\n",
      "|313ECD3A1E5BB0740...|528152BFFC925A1AE...|                    0.0|\n",
      "|488B32D24BD4BB441...|AB4A030162B75A8BC...|                    1.0|\n",
      "|488B32D24BD4BB441...|004464B10135A7A58...|                    0.0|\n",
      "+--------------------+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "hindu-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_prob(df, conditioning_rv, target):\n",
    "    \"\"\"\n",
    "    Estimate and plot conditional probability of a target being True, given a set of conditioning random variables.\n",
    "    This function is not suitable if we have attributes of type List[string], e.g. hashtags\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): pyspark DataFrame with random variables sample realizations\n",
    "        conditioning_rv (List[str]): names of the conditioning random_variables\n",
    "        target: target random variable for hìwhich parameter is being estimated\n",
    "        \n",
    "    Return:\n",
    "        result: dataframe column with conditioned probability value for each sample\n",
    "    \"\"\"\n",
    "    \n",
    "    def unwindField(row, field_name):\n",
    "        dict_row = row.asDict()\n",
    "        field = dict_row.pop(field_name, None)\n",
    "        \n",
    "        rows = []\n",
    "        if field: # If not empty\n",
    "            for el in field.split('\\t'):\n",
    "                dict_row[field_name] = el\n",
    "                rows.append(Row(**dict_row))\n",
    "\n",
    "        else:\n",
    "            dict_row[field_name] = 'None' # Instead of str('')\n",
    "            rows.append(Row(**dict_row))\n",
    "\n",
    "        return rows\n",
    "    \n",
    "\n",
    "    # Be careful, of course it can be very expensive, especially in terms of memory\n",
    "    for field in conditioning_rv:\n",
    "        if field in [\"text_tokens\", \"hashtags\", \"present_media\", \"present_links\", \"present_domains\"]:\n",
    "            df = df.rdd.flatMap(lambda row: unwindField(row, field)).toDF()\n",
    "    \n",
    "    group = ','.join(conditioning_rv)\n",
    "    df.registerTempTable(\"data\")\n",
    "    query = \"\"\"SELECT {}, mean({}) as conditional_probability\n",
    "                FROM data \n",
    "                GROUP BY {}\n",
    "            \"\"\".format(group, target, group)\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Still need statistical significance. It can be a threshold wrt to max number of occurrences\n",
    "    # Plot summary statistics\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to handle null, like no hashtag ==> simply filter them, otherwise they are an issue in summary statistics\n",
    "# How to handle List[str] attributes, like hashtags ==> simply do flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "crude-hunter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['binary_TARGET_reply_timestamp',\n",
       " 'binary_TARGET_retweet_timestamp',\n",
       " 'binary_TARGET_retweet_with_comment_timestamp',\n",
       " 'binary_TARGET_like_timestamp']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "understanding-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per quanto riguarda la statistical significance, è meglio avere una sola conditioning random variable\n",
    "# Se ne voglio piu di una, applico due volte questa funzione\n",
    "result = conditional_prob(df_mini, [\"hashtags\"], \"binary_TARGET_like_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ancient-affair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------------------+\n",
      "|hashtags                        |conditional_probability|\n",
      "+--------------------------------+-----------------------+\n",
      "|B3A404B935DF0B7213829E2B485D4AFD|0.0                    |\n",
      "|None                            |0.13636363636363635    |\n",
      "|E75B1887697C80E89F5F0F450ED8D939|0.0                    |\n",
      "|EDFA1EB79D45FB3E4C8155BCC6C8E5C9|0.0                    |\n",
      "|C56C09BD117EEB3C1077C7BA0FAEDF8F|0.0                    |\n",
      "|D424C6056B265C6B812754865337677E|0.0                    |\n",
      "|CA06BF6CCB628D70A1C7140184F2C222|0.0                    |\n",
      "|8FA0FFC783F5C16B04A2EB9DC74DB693|0.0                    |\n",
      "|311B6E08AAA492CFB5944B39583EE20F|0.0                    |\n",
      "|CDBFC509469934C1BDCFC25483D9EC72|0.0                    |\n",
      "|C849E30C938D218230221A9298A712EB|0.0                    |\n",
      "|75F737D8BEF4DCF5D3EFD6C03E5C4FAD|0.0                    |\n",
      "|6F805B6827F1526BD7D9A9276EFA0EA7|0.0                    |\n",
      "|01FF486044DFFEFFD6EF80A33E7F12AF|0.0                    |\n",
      "|BC3CD4AB80070AE2937C4F71D6612F91|0.0                    |\n",
      "|CA1C2B2272F9C610D65550D9D4D81DF7|0.0                    |\n",
      "|80E659F903FAF4194C7188BEDF507C1A|0.0                    |\n",
      "|BBCF97C76B718FAA28DA89E5AE2BD4BD|0.0                    |\n",
      "|64341CB322645AB571AB2E703B92B549|0.0                    |\n",
      "|4B2AC020C7848D773F785E71E4EBE363|0.0                    |\n",
      "+--------------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(100, truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
